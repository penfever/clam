{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYh2Yab4IsTI",
        "outputId": "eba759cd-45ce-4cb5-e4d5-63f6c70774bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openml tabpfn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V2"
      ],
      "metadata": {
        "id": "FmjVGEhRuxyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Dw3_K8FE1e",
        "outputId": "f2a0c4a9-fe5b-443f-f43f-4bf68aa88e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Detailed Analysis for Dataset: har\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-5e76fada9b5a>:24: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
            "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabPFN on 3000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tabpfn/classifier.py:422: UserWarning: Number of features 561 is greater than the maximum Number of features 500 supported by the model. You may see degraded performance.\n",
            "  X, y, feature_names_in, n_features_in = validate_Xy_fit(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabPFN accuracy - Validation: 0.9877, Test: 0.9903\n",
            "\n",
            "Extracting TabPFN embeddings...\n",
            "Raw embedding shapes - Train: (8, 3000, 192), Val: (8, 1000, 192), Test: (8, 1000, 192)\n",
            "Averaging embeddings across all ensemble members\n",
            "Processed embedding shapes - Train: (3000, 192), Val: (1000, 192), Test: (1000, 192)\n",
            "\n",
            "Embedding Statistics:\n",
            "  Mean range: [-6.7500, 5.8633]\n",
            "  Std range: [0.0782, 1.5527]\n",
            "  Overall min: -8.5547, max: 8.5703\n",
            "\n",
            "Testing different classifiers on TabPFN embeddings:\n",
            "  KNN (euclidean) - Val: 0.9900, Test: 0.9860\n",
            "  KNN (manhattan) - Val: 0.9890, Test: 0.9870\n",
            "  KNN (cosine) - Val: 0.9900, Test: 0.9860\n",
            "  SVM (RBF) - Val: 0.9890, Test: 0.9880\n",
            "  Random Forest - Val: 0.9890, Test: 0.9860\n",
            "  Neural Network - Val: 0.9910, Test: 0.9880\n",
            "\n",
            "Testing with standardized embeddings:\n",
            "  KNN (standardized) - Val: 0.9900, Test: 0.9860\n",
            "  SVM (standardized) - Val: 0.9910, Test: 0.9880\n",
            "  RF (standardized) - Val: 0.9890, Test: 0.9860\n",
            "  MLP (standardized) - Val: 0.9920, Test: 0.9880\n",
            "\n",
            "Testing PCA-reduced embeddings with varying components:\n",
            "  KNN (PCA-2, 0.6899 var) - Val: 0.9810, Test: 0.9800\n",
            "  SVM (PCA-2, 0.6899 var) - Val: 0.9790, Test: 0.9800\n",
            "  RF (PCA-2, 0.6899 var) - Val: 0.9800, Test: 0.9830\n",
            "  MLP (PCA-2, 0.6899 var) - Val: 0.9800, Test: 0.9830\n",
            "  KNN (PCA-5, 0.9070 var) - Val: 0.9870, Test: 0.9840\n",
            "  SVM (PCA-5, 0.9070 var) - Val: 0.9880, Test: 0.9870\n",
            "  RF (PCA-5, 0.9070 var) - Val: 0.9870, Test: 0.9840\n",
            "  MLP (PCA-5, 0.9070 var) - Val: 0.9890, Test: 0.9830\n",
            "  KNN (PCA-10, 0.9706 var) - Val: 0.9890, Test: 0.9860\n",
            "  SVM (PCA-10, 0.9706 var) - Val: 0.9900, Test: 0.9890\n",
            "  RF (PCA-10, 0.9706 var) - Val: 0.9880, Test: 0.9870\n",
            "  MLP (PCA-10, 0.9706 var) - Val: 0.9890, Test: 0.9860\n",
            "  KNN (PCA-20, 0.9895 var) - Val: 0.9890, Test: 0.9860\n",
            "  SVM (PCA-20, 0.9895 var) - Val: 0.9900, Test: 0.9880\n",
            "  RF (PCA-20, 0.9895 var) - Val: 0.9890, Test: 0.9880\n",
            "  MLP (PCA-20, 0.9895 var) - Val: 0.9910, Test: 0.9880\n",
            "  KNN (PCA-50, 0.9984 var) - Val: 0.9890, Test: 0.9860\n",
            "  SVM (PCA-50, 0.9984 var) - Val: 0.9910, Test: 0.9880\n",
            "  RF (PCA-50, 0.9984 var) - Val: 0.9900, Test: 0.9850\n",
            "  MLP (PCA-50, 0.9984 var) - Val: 0.9920, Test: 0.9880\n",
            "  KNN (PCA-100, 0.9998 var) - Val: 0.9900, Test: 0.9860\n",
            "  SVM (PCA-100, 0.9998 var) - Val: 0.9910, Test: 0.9880\n",
            "  RF (PCA-100, 0.9998 var) - Val: 0.9890, Test: 0.9880\n",
            "  MLP (PCA-100, 0.9998 var) - Val: 0.9910, Test: 0.9880\n",
            "\n",
            "Generating t-SNE visualization...\n",
            "Class separation analysis in t-SNE space:\n",
            "  KNN accuracy on t-SNE: 0.9860\n",
            "\n",
            "Comparing raw features vs TabPFN embeddings:\n",
            "  KNN - Raw: 0.9220, Embeddings: 0.9860, Diff: 0.0640\n",
            "  SVM - Raw: 0.9700, Embeddings: 0.9880, Diff: 0.0180\n",
            "  RF - Raw: 0.9650, Embeddings: 0.9860, Diff: 0.0210\n",
            "  MLP - Raw: 0.9780, Embeddings: 0.9880, Diff: 0.0100\n",
            "\n",
            "Confusion matrix analysis:\n",
            "Correct classifications per class (diagonal of confusion matrix):\n",
            "  TabPFN: [156 120 132 192 188 200]\n",
            "  KNN-Raw: [153 112 110 154 175 192]\n",
            "  KNN-Emb: [156 120 132 193 189 200]\n",
            "\n",
            "Analysis complete!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from tabpfn import TabPFNClassifier\n",
        "import seaborn as sns\n",
        "\n",
        "# Keeping the existing dataset loading and processing functions...\n",
        "\n",
        "def evaluate_embeddings(dataset_name, use_all_ensemble_members=True):\n",
        "    \"\"\"\n",
        "    Analyze TabPFN embeddings in detail for a dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: Name of the dataset to analyze\n",
        "        use_all_ensemble_members: Whether to combine all ensemble members or just use first one\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\\nDetailed Analysis for Dataset: {dataset_name}\\n{'='*50}\")\n",
        "\n",
        "    # Load and split dataset (reusing existing code)\n",
        "    X, y, categorical_indicator, attribute_names, full_name = load_dataset(dataset_name)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Sample if needed (TabPFN limitation)\n",
        "    max_samples = 3000\n",
        "    if len(X_train) > max_samples:\n",
        "        rng = np.random.RandomState(42)\n",
        "        train_indices = rng.choice(len(X_train), max_samples, replace=False)\n",
        "        X_train_sample = X_train[train_indices]\n",
        "        y_train_sample = y_train[train_indices]\n",
        "    else:\n",
        "        train_indices = np.arange(len(X_train))\n",
        "        X_train_sample = X_train\n",
        "        y_train_sample = y_train\n",
        "\n",
        "    # Get sample sizes for val and test sets\n",
        "    val_sample_size = min(1000, len(X_val))\n",
        "    test_sample_size = min(1000, len(X_test))\n",
        "\n",
        "    val_indices = np.random.RandomState(42).choice(len(X_val), val_sample_size, replace=False)\n",
        "    test_indices = np.random.RandomState(43).choice(len(X_test), test_sample_size, replace=False)\n",
        "\n",
        "    X_val_sample = X_val[val_indices]\n",
        "    y_val_sample = y_val[val_indices]\n",
        "    X_test_sample = X_test[test_indices]\n",
        "    y_test_sample = y_test[test_indices]\n",
        "\n",
        "    # Train TabPFN\n",
        "    N_ensemble = 8\n",
        "    tabpfn = TabPFNClassifier(device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                              n_estimators=N_ensemble,\n",
        "                              ignore_pretraining_limits=True)\n",
        "\n",
        "    print(f\"Fitting TabPFN on {len(X_train_sample)} samples\")\n",
        "    tabpfn.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "    # Get TabPFN accuracy\n",
        "    val_preds = process_in_chunks(tabpfn, X_val, chunk_size=max_samples, method='predict')\n",
        "    test_preds = process_in_chunks(tabpfn, X_test, chunk_size=max_samples, method='predict')\n",
        "\n",
        "    # Ensure we have predictions for all samples\n",
        "    y_val_eval = y_val[:len(val_preds)] if len(val_preds) < len(y_val) else y_val\n",
        "    y_test_eval = y_test[:len(test_preds)] if len(test_preds) < len(y_test) else y_test\n",
        "\n",
        "    val_accuracy = accuracy_score(y_val_eval, val_preds)\n",
        "    test_accuracy = accuracy_score(y_test_eval, test_preds)\n",
        "\n",
        "    print(f\"TabPFN accuracy - Validation: {val_accuracy:.4f}, Test: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Extract embeddings\n",
        "    print(\"\\nExtracting TabPFN embeddings...\")\n",
        "    train_emb_raw = tabpfn.get_embeddings(X_train_sample)\n",
        "    val_emb_raw = tabpfn.get_embeddings(X_val_sample)\n",
        "    test_emb_raw = tabpfn.get_embeddings(X_test_sample)\n",
        "\n",
        "    print(f\"Raw embedding shapes - Train: {train_emb_raw.shape}, Val: {val_emb_raw.shape}, Test: {test_emb_raw.shape}\")\n",
        "\n",
        "    # Process embeddings - either use first batch or combine all ensemble members\n",
        "    if use_all_ensemble_members and len(train_emb_raw.shape) == 3 and train_emb_raw.shape[0] > 1:\n",
        "        # Average across ensemble members\n",
        "        print(\"Averaging embeddings across all ensemble members\")\n",
        "        train_embeddings = np.mean(train_emb_raw, axis=0)\n",
        "        val_embeddings = np.mean(val_emb_raw, axis=0)\n",
        "        test_embeddings = np.mean(test_emb_raw, axis=0)\n",
        "    else:\n",
        "        # Use first batch/member\n",
        "        print(\"Using first ensemble member for embeddings\")\n",
        "        if len(train_emb_raw.shape) == 3:\n",
        "            train_embeddings = train_emb_raw[0]\n",
        "            val_embeddings = val_emb_raw[0]\n",
        "            test_embeddings = test_emb_raw[0]\n",
        "        else:\n",
        "            train_embeddings = train_emb_raw\n",
        "            val_embeddings = val_emb_raw\n",
        "            test_embeddings = test_emb_raw\n",
        "\n",
        "    print(f\"Processed embedding shapes - Train: {train_embeddings.shape}, Val: {val_embeddings.shape}, Test: {test_embeddings.shape}\")\n",
        "\n",
        "    # Calculate and visualize embedding statistics\n",
        "    print(\"\\nEmbedding Statistics:\")\n",
        "    emb_mean = np.mean(train_embeddings, axis=0)\n",
        "    emb_std = np.std(train_embeddings, axis=0)\n",
        "    print(f\"  Mean range: [{np.min(emb_mean):.4f}, {np.max(emb_mean):.4f}]\")\n",
        "    print(f\"  Std range: [{np.min(emb_std):.4f}, {np.max(emb_std):.4f}]\")\n",
        "    print(f\"  Overall min: {np.min(train_embeddings):.4f}, max: {np.max(train_embeddings):.4f}\")\n",
        "\n",
        "    # Try multiple classifiers on the embeddings\n",
        "    print(\"\\nTesting different classifiers on TabPFN embeddings:\")\n",
        "\n",
        "    # 1. KNN with different metrics\n",
        "    k = max(5, int(np.sqrt(len(train_embeddings))))\n",
        "\n",
        "    for metric in ['euclidean', 'manhattan', 'cosine']:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
        "        knn.fit(train_embeddings, y_train_sample)\n",
        "        val_acc = accuracy_score(y_val_sample, knn.predict(val_embeddings))\n",
        "        test_acc = accuracy_score(y_test_sample, knn.predict(test_embeddings))\n",
        "        print(f\"  KNN ({metric}) - Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # 2. SVM classifier\n",
        "    svm = SVC(kernel='rbf')\n",
        "    svm.fit(train_embeddings, y_train_sample)\n",
        "    val_acc = accuracy_score(y_val_sample, svm.predict(val_embeddings))\n",
        "    test_acc = accuracy_score(y_test_sample, svm.predict(test_embeddings))\n",
        "    print(f\"  SVM (RBF) - Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # 3. Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(train_embeddings, y_train_sample)\n",
        "    val_acc = accuracy_score(y_val_sample, rf.predict(val_embeddings))\n",
        "    test_acc = accuracy_score(y_test_sample, rf.predict(test_embeddings))\n",
        "    print(f\"  Random Forest - Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # 4. Neural Network\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
        "    mlp.fit(train_embeddings, y_train_sample)\n",
        "    val_acc = accuracy_score(y_val_sample, mlp.predict(val_embeddings))\n",
        "    test_acc = accuracy_score(y_test_sample, mlp.predict(test_embeddings))\n",
        "    print(f\"  Neural Network - Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # 5. Try with standardized embeddings\n",
        "    print(\"\\nTesting with standardized embeddings:\")\n",
        "    scaler = StandardScaler()\n",
        "    train_emb_scaled = scaler.fit_transform(train_embeddings)\n",
        "    val_emb_scaled = scaler.transform(val_embeddings)\n",
        "    test_emb_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "    for model_name, model in [\n",
        "        (\"KNN\", KNeighborsClassifier(n_neighbors=k)),\n",
        "        (\"SVM\", SVC(kernel='rbf')),\n",
        "        (\"RF\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42))\n",
        "    ]:\n",
        "        model.fit(train_emb_scaled, y_train_sample)\n",
        "        val_acc = accuracy_score(y_val_sample, model.predict(val_emb_scaled))\n",
        "        test_acc = accuracy_score(y_test_sample, model.predict(test_emb_scaled))\n",
        "        print(f\"  {model_name} (standardized) - Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # 6. Try with PCA-reduced embeddings\n",
        "    print(\"\\nTesting PCA-reduced embeddings with varying components:\")\n",
        "\n",
        "    # Try different numbers of components\n",
        "    n_components_list = [2, 5, 10, 20, 50, 100]\n",
        "\n",
        "    for n_components in n_components_list:\n",
        "        if n_components >= min(train_embeddings.shape):\n",
        "            continue\n",
        "\n",
        "        pca = PCA(n_components=n_components)\n",
        "        train_emb_pca = pca.fit_transform(train_embeddings)\n",
        "        val_emb_pca = pca.transform(val_embeddings)\n",
        "        test_emb_pca = pca.transform(test_embeddings)\n",
        "\n",
        "        # Calculate variance explained\n",
        "        var_explained = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "        for model_name, model in [\n",
        "            (\"KNN\", KNeighborsClassifier(n_neighbors=k)),\n",
        "            (\"SVM\", SVC(kernel='rbf')),\n",
        "            (\"RF\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42))\n",
        "        ]:\n",
        "            model.fit(train_emb_pca, y_train_sample)\n",
        "            val_acc = accuracy_score(y_val_sample, model.predict(val_emb_pca))\n",
        "            test_acc = accuracy_score(y_test_sample, model.predict(test_emb_pca))\n",
        "            print(f\"  {model_name} (PCA-{n_components}, {var_explained:.4f} var) - Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # 7. Visualize a few examples with t-SNE\n",
        "    print(\"\\nGenerating t-SNE visualization...\")\n",
        "\n",
        "    # Combine train and validation for visualization\n",
        "    combined_emb = np.vstack([train_embeddings[:500], val_embeddings[:500]])  # Limit size for speed\n",
        "    combined_labels = np.concatenate([y_train_sample[:500], y_val_sample[:500]])\n",
        "\n",
        "    # Apply t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    embedded = tsne.fit_transform(combined_emb)\n",
        "\n",
        "    # Split back\n",
        "    train_embedded = embedded[:500]\n",
        "    val_embedded = embedded[500:]\n",
        "\n",
        "    # Analyze class separation in t-SNE space\n",
        "    print(\"Class separation analysis in t-SNE space:\")\n",
        "    knn_tsne = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_tsne.fit(train_embedded, y_train_sample[:500])\n",
        "    val_acc_tsne = accuracy_score(y_val_sample[:500], knn_tsne.predict(val_embedded))\n",
        "    print(f\"  KNN accuracy on t-SNE: {val_acc_tsne:.4f}\")\n",
        "\n",
        "    # Analyze raw vs. TabPFN embeddings\n",
        "    print(\"\\nComparing raw features vs TabPFN embeddings:\")\n",
        "\n",
        "    # Normalize raw features\n",
        "    scaler_raw = StandardScaler()\n",
        "    X_train_scaled = scaler_raw.fit_transform(X_train_sample)\n",
        "    X_val_scaled = scaler_raw.transform(X_val_sample)\n",
        "    X_test_scaled = scaler_raw.transform(X_test_sample)\n",
        "\n",
        "    # Compare on same models\n",
        "    for model_name, model_raw, model_emb in [\n",
        "        (\"KNN\", KNeighborsClassifier(n_neighbors=k), KNeighborsClassifier(n_neighbors=k)),\n",
        "        (\"SVM\", SVC(kernel='rbf'), SVC(kernel='rbf')),\n",
        "        (\"RF\", RandomForestClassifier(n_estimators=100, random_state=42), RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42), MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42))\n",
        "    ]:\n",
        "        # Raw features\n",
        "        model_raw.fit(X_train_scaled, y_train_sample)\n",
        "        raw_val_acc = accuracy_score(y_val_sample, model_raw.predict(X_val_scaled))\n",
        "        raw_test_acc = accuracy_score(y_test_sample, model_raw.predict(X_test_scaled))\n",
        "\n",
        "        # Embeddings (scaled)\n",
        "        model_emb.fit(train_emb_scaled, y_train_sample)\n",
        "        emb_val_acc = accuracy_score(y_val_sample, model_emb.predict(val_emb_scaled))\n",
        "        emb_test_acc = accuracy_score(y_test_sample, model_emb.predict(test_emb_scaled))\n",
        "\n",
        "        print(f\"  {model_name} - Raw: {raw_test_acc:.4f}, Embeddings: {emb_test_acc:.4f}, Diff: {emb_test_acc-raw_test_acc:.4f}\")\n",
        "\n",
        "    # Analyze confusion matrices\n",
        "    print(\"\\nConfusion matrix analysis:\")\n",
        "\n",
        "    # Get predictions from TabPFN and KNN\n",
        "    tabpfn_val_preds = tabpfn.predict(X_val_sample)\n",
        "\n",
        "    knn_raw = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_raw.fit(X_train_scaled, y_train_sample)\n",
        "    knn_raw_val_preds = knn_raw.predict(X_val_scaled)\n",
        "\n",
        "    knn_emb = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_emb.fit(train_emb_scaled, y_train_sample)\n",
        "    knn_emb_val_preds = knn_emb.predict(val_emb_scaled)\n",
        "\n",
        "    # Calculate confusion matrices\n",
        "    cm_tabpfn = confusion_matrix(y_val_sample, tabpfn_val_preds)\n",
        "    cm_knn_raw = confusion_matrix(y_val_sample, knn_raw_val_preds)\n",
        "    cm_knn_emb = confusion_matrix(y_val_sample, knn_emb_val_preds)\n",
        "\n",
        "    # Print diagonal elements (correct classifications per class)\n",
        "    print(\"Correct classifications per class (diagonal of confusion matrix):\")\n",
        "    print(\"  TabPFN:\", np.diag(cm_tabpfn))\n",
        "    print(\"  KNN-Raw:\", np.diag(cm_knn_raw))\n",
        "    print(\"  KNN-Emb:\", np.diag(cm_knn_emb))\n",
        "\n",
        "    return {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"tabpfn_val_acc\": val_accuracy,\n",
        "        \"tabpfn_test_acc\": test_accuracy,\n",
        "        \"embeddings_shape\": train_embeddings.shape,\n",
        "        # Add other metrics as needed\n",
        "    }\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    import torch\n",
        "\n",
        "    # Fix random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Analyze a single dataset in detail\n",
        "    dataset_name = 'har'  # Replace with dataset of interest\n",
        "    results = evaluate_embeddings(dataset_name)\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revised Version of V1"
      ],
      "metadata": {
        "id": "mylwQosWul8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n",
        "# Load datasets from OpenML (keeping same function)\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name == 'airlines':\n",
        "        dataset = openml.datasets.get_dataset(1169)  # Airlines dataset\n",
        "    elif dataset_name == 'albert':\n",
        "        dataset = openml.datasets.get_dataset(189356)  # Albert dataset\n",
        "    elif dataset_name == 'volkert':\n",
        "        dataset = openml.datasets.get_dataset(41166)  # Volkert dataset\n",
        "    elif dataset_name == 'higgs':\n",
        "        dataset = openml.datasets.get_dataset(44129)  # Higgs dataset\n",
        "    elif dataset_name == 'har':\n",
        "        dataset = openml.datasets.get_dataset(1478)  # Har dataset\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
        "        dataset_format=\"array\", target=dataset.default_target_attribute\n",
        "    )\n",
        "\n",
        "    return X, y, categorical_indicator, attribute_names, dataset.name\n",
        "\n",
        "# Process data in chunks (improved with consistent error handling)\n",
        "def process_in_chunks(model, X, chunk_size=3000, method='predict'):\n",
        "    \"\"\"Process large datasets in chunks of maximum size 3000 (TabPFN limitation)\"\"\"\n",
        "    n_samples = len(X)\n",
        "    n_chunks = int(np.ceil(n_samples / chunk_size))\n",
        "\n",
        "    all_results = []\n",
        "    first_result_shape = None\n",
        "\n",
        "    for i in range(n_chunks):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min((i + 1) * chunk_size, n_samples)\n",
        "\n",
        "        X_chunk = X[start_idx:end_idx]\n",
        "\n",
        "        try:\n",
        "            # Process the chunk\n",
        "            if method == 'predict':\n",
        "                chunk_result = model.predict(X_chunk)\n",
        "            elif method == 'get_embeddings':\n",
        "                chunk_result = model.get_embeddings(X_chunk)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "            # Store the shape of the first result\n",
        "            if i == 0:\n",
        "                first_result_shape = chunk_result.shape\n",
        "\n",
        "                # For 2D results, we only care about the second dimension onwards\n",
        "                if len(first_result_shape) > 1:\n",
        "                    feature_dims = first_result_shape[1:]\n",
        "                else:\n",
        "                    feature_dims = ()\n",
        "\n",
        "            all_results.append(chunk_result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk {i}: {e}\")\n",
        "            # Return partial results if we have some\n",
        "            if all_results:\n",
        "                break\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    # Combine results safely\n",
        "    try:\n",
        "        return np.concatenate(all_results)\n",
        "    except ValueError as e:\n",
        "        print(f\"Concatenation error: {e}\")\n",
        "\n",
        "        # For 1D arrays (like predictions), just concatenate what we have\n",
        "        if all_results and len(all_results[0].shape) == 1:\n",
        "            result = np.concatenate([r for r in all_results if len(r) > 0])\n",
        "        else:\n",
        "            # For 2D arrays, ensure consistent dimensions\n",
        "            if all_results and len(all_results[0].shape) > 1:\n",
        "                feature_dim = all_results[0].shape[1]\n",
        "                valid_results = [r for r in all_results if r.shape[1] == feature_dim]\n",
        "\n",
        "                if not valid_results:\n",
        "                    raise ValueError(\"No valid results to concatenate\")\n",
        "\n",
        "                result = np.concatenate(valid_results)\n",
        "            else:\n",
        "                raise ValueError(\"Unable to concatenate results\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# Improved KNN classifier evaluation\n",
        "def evaluate_knn_classifier(X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                            train_embeddings=None, val_embeddings=None, test_embeddings=None):\n",
        "    \"\"\"\n",
        "    Evaluate KNN classifier performance on both raw features and embeddings.\n",
        "    Ensures consistent sample handling and proper alignment of predictions.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # 1. KNN on raw features (with proper standardization)\n",
        "    print(\"\\nEvaluating KNN on raw features:\")\n",
        "\n",
        "    # Set k to be sqrt(n) or at least 5\n",
        "    k = max(5, int(np.sqrt(len(X_train))))\n",
        "    print(f\"Using k={k} for KNN classification\")\n",
        "\n",
        "    # Standardize the data (important for KNN)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train KNN classifier on raw features\n",
        "    knn_raw = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_raw.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_preds_knn_raw = knn_raw.predict(X_val_scaled)\n",
        "    val_accuracy_knn_raw = accuracy_score(y_val, val_preds_knn_raw)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_preds_knn_raw = knn_raw.predict(X_test_scaled)\n",
        "    test_accuracy_knn_raw = accuracy_score(y_test, test_preds_knn_raw)\n",
        "\n",
        "    print(f\"  Validation accuracy (raw features): {val_accuracy_knn_raw:.4f}\")\n",
        "    print(f\"  Test accuracy (raw features): {test_accuracy_knn_raw:.4f}\")\n",
        "\n",
        "    results['knn_raw_val_accuracy'] = val_accuracy_knn_raw\n",
        "    results['knn_raw_test_accuracy'] = test_accuracy_knn_raw\n",
        "\n",
        "    # 2. KNN on embeddings (if provided)\n",
        "    if train_embeddings is not None and val_embeddings is not None and test_embeddings is not None:\n",
        "        print(\"\\nEvaluating KNN on TabPFN embeddings:\")\n",
        "\n",
        "        # Make sure we have exactly matching samples and labels\n",
        "        assert len(train_embeddings) <= len(y_train), \"Too many training embeddings\"\n",
        "        assert len(val_embeddings) <= len(y_val), \"Too many validation embeddings\"\n",
        "        assert len(test_embeddings) <= len(y_test), \"Too many test embeddings\"\n",
        "\n",
        "        # Get corresponding labels for the embeddings\n",
        "        y_train_emb = y_train[:len(train_embeddings)]\n",
        "        y_val_emb = y_val[:len(val_embeddings)]\n",
        "        y_test_emb = y_test[:len(test_embeddings)]\n",
        "\n",
        "        print(f\"  Using {len(y_train_emb)} training labels, {len(y_val_emb)} validation labels, {len(y_test_emb)} test labels\")\n",
        "        print(f\"  Label distribution in train: {np.unique(y_train_emb, return_counts=True)}\")\n",
        "\n",
        "        # Standardize the embeddings\n",
        "        scaler_emb = StandardScaler()\n",
        "        train_embeddings_std = scaler_emb.fit_transform(train_embeddings)\n",
        "        val_embeddings_std = scaler_emb.transform(val_embeddings)\n",
        "        test_embeddings_std = scaler_emb.transform(test_embeddings)\n",
        "\n",
        "        # Train KNN classifier on standardized embeddings\n",
        "        knn_emb = KNeighborsClassifier(n_neighbors=k)\n",
        "        knn_emb.fit(train_embeddings_std, y_train_emb)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_preds_knn_emb = knn_emb.predict(val_embeddings_std)\n",
        "        val_accuracy_knn_emb = accuracy_score(y_val_emb, val_preds_knn_emb)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_preds_knn_emb = knn_emb.predict(test_embeddings_std)\n",
        "        test_accuracy_knn_emb = accuracy_score(y_test_emb, test_preds_knn_emb)\n",
        "\n",
        "        print(f\"  Validation accuracy (standardized embeddings): {val_accuracy_knn_emb:.4f}\")\n",
        "        print(f\"  Test accuracy (standardized embeddings): {test_accuracy_knn_emb:.4f}\")\n",
        "\n",
        "        results['knn_emb_val_accuracy'] = val_accuracy_knn_emb\n",
        "        results['knn_emb_test_accuracy'] = test_accuracy_knn_emb\n",
        "\n",
        "        # 3. KNN on PCA-reduced embeddings with specific component counts\n",
        "        print(\"\\nEvaluating KNN on PCA-reduced TabPFN embeddings:\")\n",
        "\n",
        "        # Use specific component counts (like in evaluate_embeddings) instead of percentages\n",
        "        pca_components = [2, 5, 10, 20, 50, 100]\n",
        "        pca_components = [c for c in pca_components if c < train_embeddings.shape[1]]\n",
        "\n",
        "        best_val_accuracy = 0\n",
        "        best_test_accuracy = 0\n",
        "        best_n_components = 0\n",
        "\n",
        "        print(f\"  Testing PCA components: {pca_components}\")\n",
        "\n",
        "        for n_components in pca_components:\n",
        "            try:\n",
        "                # Apply PCA to standardized embeddings\n",
        "                pca = PCA(n_components=n_components, random_state=42)  # Set random state for reproducibility\n",
        "                train_embeddings_pca = pca.fit_transform(train_embeddings_std)\n",
        "                val_embeddings_pca = pca.transform(val_embeddings_std)\n",
        "                test_embeddings_pca = pca.transform(test_embeddings_std)\n",
        "\n",
        "                # Train KNN on PCA-reduced embeddings\n",
        "                knn_pca = KNeighborsClassifier(n_neighbors=k)\n",
        "                knn_pca.fit(train_embeddings_pca, y_train_emb)\n",
        "\n",
        "                # Evaluate on validation set\n",
        "                val_preds_knn_pca = knn_pca.predict(val_embeddings_pca)\n",
        "                val_accuracy_knn_pca = accuracy_score(y_val_emb, val_preds_knn_pca)\n",
        "\n",
        "                # Evaluate on test set\n",
        "                test_preds_knn_pca = knn_pca.predict(test_embeddings_pca)\n",
        "                test_accuracy_knn_pca = accuracy_score(y_test_emb, test_preds_knn_pca)\n",
        "\n",
        "                # Calculate variance explained\n",
        "                var_explained = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "                print(f\"  PCA components={n_components}: Val accuracy={val_accuracy_knn_pca:.4f}, \"\n",
        "                      f\"Test accuracy={test_accuracy_knn_pca:.4f}, Variance explained={var_explained:.4f}\")\n",
        "\n",
        "                # Track best validation performance\n",
        "                if val_accuracy_knn_pca > best_val_accuracy:\n",
        "                    best_val_accuracy = val_accuracy_knn_pca\n",
        "                    best_test_accuracy = test_accuracy_knn_pca\n",
        "                    best_n_components = n_components\n",
        "                    best_variance = var_explained\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error with n_components={n_components}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Record best PCA results\n",
        "        if best_n_components > 0:\n",
        "            print(f\"  Best PCA results: components={best_n_components}, \"\n",
        "                  f\"val_accuracy={best_val_accuracy:.4f}, test_accuracy={best_test_accuracy:.4f}\")\n",
        "            results['knn_pca_best_components'] = best_n_components\n",
        "            results['knn_pca_val_accuracy'] = best_val_accuracy\n",
        "            results['knn_pca_test_accuracy'] = best_test_accuracy\n",
        "            results['pca_variance_explained'] = best_variance\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main analysis function with consistent random seed handling\n",
        "def analyze_dataset(dataset_name):\n",
        "    print(f\"\\n{'='*50}\\nAnalyzing dataset: {dataset_name}\\n{'='*50}\")\n",
        "\n",
        "    # Fix random seed for all random operations\n",
        "    random_seed = 42\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Load dataset\n",
        "    X, y, categorical_indicator, attribute_names, full_name = load_dataset(dataset_name)\n",
        "\n",
        "    # Split data into train/val/test (70/15/15) with fixed random state\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_seed)\n",
        "\n",
        "    print(f\"Dataset shapes:\")\n",
        "    print(f\"  Train: {X_train.shape}, {np.unique(y_train, return_counts=True)}\")\n",
        "    print(f\"  Val:   {X_val.shape}, {np.unique(y_val, return_counts=True)}\")\n",
        "    print(f\"  Test:  {X_test.shape}, {np.unique(y_test, return_counts=True)}\")\n",
        "\n",
        "    # TabPFN can only handle 3000 samples at a time\n",
        "    max_samples = 3000\n",
        "\n",
        "    # Use RandomState for consistent sampling\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "\n",
        "    # Randomly select samples if we have more than max_samples\n",
        "    if len(X_train) > max_samples:\n",
        "        # Random selection with fixed seed for reproducibility\n",
        "        train_indices = rng.choice(len(X_train), max_samples, replace=False)\n",
        "        X_train_sample = X_train[train_indices]\n",
        "        y_train_sample = y_train[train_indices]\n",
        "        print(f\"Randomly selected {max_samples} samples from {len(X_train)} training samples\")\n",
        "    else:\n",
        "        train_indices = np.arange(len(X_train))\n",
        "        X_train_sample = X_train\n",
        "        y_train_sample = y_train\n",
        "        print(f\"Using all {len(X_train)} training samples\")\n",
        "\n",
        "    # 3. Load TabPFN and get predictions\n",
        "    N_ensemble = 8  # Number of models in ensemble\n",
        "    try:\n",
        "        import torch\n",
        "        tabpfn = TabPFNClassifier(device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                                n_estimators=N_ensemble,\n",
        "                                ignore_pretraining_limits=True)\n",
        "    except ImportError:\n",
        "        # Fallback if no torch\n",
        "        tabpfn = TabPFNClassifier(n_estimators=N_ensemble,\n",
        "                                ignore_pretraining_limits=True)\n",
        "\n",
        "    # Fit TabPFN on the selected samples\n",
        "    print(f\"Fitting TabPFN on {len(X_train_sample)} samples\")\n",
        "    tabpfn.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "    # Get predictions on validation and test sets\n",
        "    try:\n",
        "        val_preds = tabpfn.predict(X_val)\n",
        "        val_accuracy = accuracy_score(y_val, val_preds)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during validation prediction: {e}\")\n",
        "        # Try chunked prediction as fallback\n",
        "        try:\n",
        "            val_preds = process_in_chunks(tabpfn, X_val, chunk_size=max_samples, method='predict')\n",
        "            val_accuracy = accuracy_score(y_val[:len(val_preds)], val_preds)\n",
        "        except Exception as e2:\n",
        "            print(f\"Chunked prediction also failed: {e2}\")\n",
        "            val_accuracy = np.nan\n",
        "\n",
        "    try:\n",
        "        test_preds = tabpfn.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test, test_preds)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during test prediction: {e}\")\n",
        "        # Try chunked prediction as fallback\n",
        "        try:\n",
        "            test_preds = process_in_chunks(tabpfn, X_test, chunk_size=max_samples, method='predict')\n",
        "            test_accuracy = accuracy_score(y_test[:len(test_preds)], test_preds)\n",
        "        except Exception as e2:\n",
        "            print(f\"Chunked prediction also failed: {e2}\")\n",
        "            test_accuracy = np.nan\n",
        "\n",
        "    print(f\"TabPFN on initial training set:\")\n",
        "    print(f\"  Validation accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"  Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # 4. Extract embeddings from TabPFN with consistent sampling\n",
        "    print(\"Extracting embeddings from TabPFN\")\n",
        "\n",
        "    # Use consistent sampling with reproducible RandomState\n",
        "    val_sample_size = min(1000, len(X_val))\n",
        "    val_indices = rng.choice(len(X_val), val_sample_size, replace=False)\n",
        "    X_val_sample = X_val[val_indices]\n",
        "    y_val_sample = y_val[val_indices]\n",
        "\n",
        "    test_sample_size = min(1000, len(X_test))\n",
        "    test_indices = rng.choice(len(X_test), test_sample_size, replace=False)\n",
        "    X_test_sample = X_test[test_indices]\n",
        "    y_test_sample = y_test[test_indices]\n",
        "\n",
        "    # Get embeddings\n",
        "    train_embeddings_raw = tabpfn.get_embeddings(X_train_sample)\n",
        "    val_embeddings_raw = tabpfn.get_embeddings(X_val_sample)\n",
        "    test_embeddings_raw = tabpfn.get_embeddings(X_test_sample)\n",
        "\n",
        "    print(f\"Embeddings shapes - Train: {train_embeddings_raw.shape}, \"\n",
        "          f\"Val: {val_embeddings_raw.shape}, Test: {test_embeddings_raw.shape}\")\n",
        "\n",
        "    # Process embeddings - average across ensemble members if available\n",
        "    if len(train_embeddings_raw.shape) == 3 and train_embeddings_raw.shape[0] > 1:\n",
        "        print(\"Averaging embeddings across ensemble members\")\n",
        "        train_embeddings = np.mean(train_embeddings_raw, axis=0)\n",
        "        val_embeddings = np.mean(val_embeddings_raw, axis=0)\n",
        "        test_embeddings = np.mean(test_embeddings_raw, axis=0)\n",
        "    else:\n",
        "        # For backward compatibility, handle original format\n",
        "        if len(train_embeddings_raw.shape) == 3:\n",
        "            train_embeddings = train_embeddings_raw[0]\n",
        "            val_embeddings = val_embeddings_raw[0]\n",
        "            test_embeddings = test_embeddings_raw[0]\n",
        "        else:\n",
        "            train_embeddings = train_embeddings_raw\n",
        "            val_embeddings = val_embeddings_raw\n",
        "            test_embeddings = test_embeddings_raw\n",
        "\n",
        "    print(f\"Processed embedding shapes - Train: {train_embeddings.shape}, \"\n",
        "          f\"Val: {val_embeddings.shape}, Test: {test_embeddings.shape}\")\n",
        "\n",
        "    # Check that all embeddings have the same feature dimension\n",
        "    if not (train_embeddings.shape[1] == val_embeddings.shape[1] == test_embeddings.shape[1]):\n",
        "        print(f\"WARNING: Feature dimensions don't match! Train: {train_embeddings.shape[1]}, \" +\n",
        "              f\"Val: {val_embeddings.shape[1]}, Test: {test_embeddings.shape[1]}\")\n",
        "\n",
        "    # Evaluate KNN classifier with the properly processed embeddings\n",
        "    knn_results = evaluate_knn_classifier(\n",
        "        X_train_sample, y_train_sample,\n",
        "        X_val_sample, y_val_sample,\n",
        "        X_test_sample, y_test_sample,\n",
        "        train_embeddings, val_embeddings, test_embeddings\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'dataset': full_name,\n",
        "        'full_train_size': len(X_train),\n",
        "        'sample_train_size': len(X_train_sample),\n",
        "        'full_val_accuracy': val_accuracy,\n",
        "        'full_test_accuracy': test_accuracy,\n",
        "        # Add KNN results\n",
        "        'knn_raw_val_accuracy': knn_results.get('knn_raw_val_accuracy', np.nan),\n",
        "        'knn_raw_test_accuracy': knn_results.get('knn_raw_test_accuracy', np.nan),\n",
        "        'knn_emb_val_accuracy': knn_results.get('knn_emb_val_accuracy', np.nan),\n",
        "        'knn_emb_test_accuracy': knn_results.get('knn_emb_test_accuracy', np.nan),\n",
        "        'knn_pca_best_components': knn_results.get('knn_pca_best_components', np.nan),\n",
        "        'knn_pca_val_accuracy': knn_results.get('knn_pca_val_accuracy', np.nan),\n",
        "        'knn_pca_test_accuracy': knn_results.get('knn_pca_test_accuracy', np.nan),\n",
        "        'pca_variance_explained': knn_results.get('pca_variance_explained', np.nan)\n",
        "    }\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    import torch\n",
        "    import pandas as pd\n",
        "\n",
        "    # Fix random seed for reproducibility\n",
        "    random_seed = 42\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    dataset_names = ['har', 'volkert', 'higgs', 'airlines', 'albert']\n",
        "\n",
        "    # Store results\n",
        "    results = []\n",
        "\n",
        "    # Process each dataset\n",
        "    for dataset_name in tqdm(dataset_names, desc=\"Processing datasets\"):\n",
        "        try:\n",
        "            result = analyze_dataset(dataset_name)\n",
        "            results.append(result)\n",
        "            print(\"\\n\\n=== SUMMARY OF RESULTS ===\")\n",
        "            cols_to_show = ['dataset', 'full_test_accuracy',\n",
        "                          'knn_raw_test_accuracy', 'knn_emb_test_accuracy',\n",
        "                          'knn_pca_best_components', 'knn_pca_test_accuracy', 'pca_variance_explained']\n",
        "\n",
        "            # Convert result to DataFrame for display\n",
        "            results_df = pd.DataFrame([result])\n",
        "            print(results_df[cols_to_show])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing dataset {dataset_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Create summary dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('/content/drive/MyDrive/tabpfn-knn-results.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7XtjsFuoZp",
        "outputId": "febbbf87-2ca1-403f-97f3-851bd92608f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing datasets:   0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-10-e378f9beba3e>:25: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
            "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Analyzing dataset: har\n",
            "==================================================\n",
            "Dataset shapes:\n",
            "  Train: (7209, 561), (array([0, 1, 2, 3, 4, 5]), array([1183, 1120,  986, 1214, 1359, 1347]))\n",
            "  Val:   (1545, 561), (array([0, 1, 2, 3, 4, 5]), array([255, 198, 217, 294, 280, 301]))\n",
            "  Test:  (1545, 561), (array([0, 1, 2, 3, 4, 5]), array([284, 226, 203, 269, 267, 296]))\n",
            "Randomly selected 3000 samples from 7209 training samples\n",
            "Fitting TabPFN on 3000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tabpfn/classifier.py:422: UserWarning: Number of features 561 is greater than the maximum Number of features 500 supported by the model. You may see degraded performance.\n",
            "  X, y, feature_names_in, n_features_in = validate_Xy_fit(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabPFN on initial training set:\n",
            "  Validation accuracy: 0.9877\n",
            "  Test accuracy: 0.9903\n",
            "Extracting embeddings from TabPFN\n",
            "Embeddings shapes - Train: (8, 3000, 192), Val: (8, 1000, 192), Test: (8, 1000, 192)\n",
            "Averaging embeddings across ensemble members\n",
            "Processed embedding shapes - Train: (3000, 192), Val: (1000, 192), Test: (1000, 192)\n",
            "\n",
            "Evaluating KNN on raw features:\n",
            "Using k=54 for KNN classification\n",
            "  Validation accuracy (raw features): 0.8920\n",
            "  Test accuracy (raw features): 0.9100\n",
            "\n",
            "Evaluating KNN on TabPFN embeddings:\n",
            "  Using 3000 training labels, 1000 validation labels, 1000 test labels\n",
            "  Label distribution in train: (array([0, 1, 2, 3, 4, 5]), array([513, 446, 403, 511, 569, 558]))\n",
            "  Validation accuracy (standardized embeddings): 0.9900\n",
            "  Test accuracy (standardized embeddings): 0.9870\n",
            "\n",
            "Evaluating KNN on PCA-reduced TabPFN embeddings:\n",
            "  Testing PCA components: [2, 5, 10, 20, 50, 100]\n",
            "  PCA components=2: Val accuracy=0.9200, Test accuracy=0.9290, Variance explained=0.5309\n",
            "  PCA components=5: Val accuracy=0.9810, Test accuracy=0.9810, Variance explained=0.8076\n",
            "  PCA components=10: Val accuracy=0.9900, Test accuracy=0.9860, Variance explained=0.9314\n",
            "  PCA components=20: Val accuracy=0.9900, Test accuracy=0.9870, Variance explained=0.9753\n",
            "  PCA components=50: Val accuracy=0.9900, Test accuracy=0.9870, Variance explained=0.9967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing datasets:  20%|██        | 1/5 [10:39<42:38, 639.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PCA components=100: Val accuracy=0.9900, Test accuracy=0.9870, Variance explained=0.9996\n",
            "  Best PCA results: components=10, val_accuracy=0.9900, test_accuracy=0.9860\n",
            "\n",
            "\n",
            "=== SUMMARY OF RESULTS ===\n",
            "  dataset  full_test_accuracy  knn_raw_test_accuracy  knn_emb_test_accuracy  \\\n",
            "0     har            0.990291                   0.91                  0.987   \n",
            "\n",
            "   knn_pca_best_components  knn_pca_test_accuracy  pca_variance_explained  \n",
            "0                       10                  0.986                0.931394  \n",
            "\n",
            "==================================================\n",
            "Analyzing dataset: volkert\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e378f9beba3e>:25: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
            "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes:\n",
            "  Train: (40817, 180), (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([5111, 1228, 8040, 2043, 2464, 6644, 1050, 8948,  919, 4370]))\n",
            "  Val:   (8746, 180), (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([1127,  257, 1755,  437,  518, 1370,  223, 1947,  214,  898]))\n",
            "  Test:  (8747, 180), (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([1140,  280, 1724,  450,  517, 1400,  207, 1911,  228,  890]))\n",
            "Randomly selected 3000 samples from 40817 training samples\n",
            "Fitting TabPFN on 3000 samples\n",
            "TabPFN on initial training set:\n",
            "  Validation accuracy: 0.6062\n",
            "  Test accuracy: 0.5941\n",
            "Extracting embeddings from TabPFN\n",
            "Embeddings shapes - Train: (8, 3000, 192), Val: (8, 1000, 192), Test: (8, 1000, 192)\n",
            "Averaging embeddings across ensemble members\n",
            "Processed embedding shapes - Train: (3000, 192), Val: (1000, 192), Test: (1000, 192)\n",
            "\n",
            "Evaluating KNN on raw features:\n",
            "Using k=54 for KNN classification\n",
            "  Validation accuracy (raw features): 0.5000\n",
            "  Test accuracy (raw features): 0.5150\n",
            "\n",
            "Evaluating KNN on TabPFN embeddings:\n",
            "  Using 3000 training labels, 1000 validation labels, 1000 test labels\n",
            "  Label distribution in train: (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([352,  86, 617, 142, 173, 485,  98, 659,  74, 314]))\n",
            "  Validation accuracy (standardized embeddings): 0.6150\n",
            "  Test accuracy (standardized embeddings): 0.5880\n",
            "\n",
            "Evaluating KNN on PCA-reduced TabPFN embeddings:\n",
            "  Testing PCA components: [2, 5, 10, 20, 50, 100]\n",
            "  PCA components=2: Val accuracy=0.5260, Test accuracy=0.5000, Variance explained=0.5508\n",
            "  PCA components=5: Val accuracy=0.5780, Test accuracy=0.5760, Variance explained=0.7948\n",
            "  PCA components=10: Val accuracy=0.5900, Test accuracy=0.5840, Variance explained=0.9041\n",
            "  PCA components=20: Val accuracy=0.6090, Test accuracy=0.5870, Variance explained=0.9680\n",
            "  PCA components=50: Val accuracy=0.6140, Test accuracy=0.5870, Variance explained=0.9964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing datasets:  40%|████      | 2/5 [14:52<20:35, 411.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PCA components=100: Val accuracy=0.6150, Test accuracy=0.5880, Variance explained=0.9996\n",
            "  Best PCA results: components=100, val_accuracy=0.6150, test_accuracy=0.5880\n",
            "\n",
            "\n",
            "=== SUMMARY OF RESULTS ===\n",
            "   dataset  full_test_accuracy  knn_raw_test_accuracy  knn_emb_test_accuracy  \\\n",
            "0  volkert            0.594147                  0.515                  0.588   \n",
            "\n",
            "   knn_pca_best_components  knn_pca_test_accuracy  pca_variance_explained  \n",
            "0                      100                  0.588                0.999576  \n",
            "\n",
            "==================================================\n",
            "Analyzing dataset: higgs\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e378f9beba3e>:25: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
            "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes:\n",
            "  Train: (658112, 24), (array([0, 1]), array([329118, 328994]))\n",
            "  Val:   (141024, 24), (array([0, 1]), array([70435, 70589]))\n",
            "  Test:  (141024, 24), (array([0, 1]), array([70527, 70497]))\n",
            "Randomly selected 3000 samples from 658112 training samples\n",
            "Fitting TabPFN on 3000 samples\n",
            "Error during validation prediction: CUDA error: invalid configuration argument\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Error during test prediction: CUDA error: invalid configuration argument\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "TabPFN on initial training set:\n",
            "  Validation accuracy: 0.7164\n",
            "  Test accuracy: 0.7145\n",
            "Extracting embeddings from TabPFN\n",
            "Embeddings shapes - Train: (8, 3000, 192), Val: (8, 1000, 192), Test: (8, 1000, 192)\n",
            "Averaging embeddings across ensemble members\n",
            "Processed embedding shapes - Train: (3000, 192), Val: (1000, 192), Test: (1000, 192)\n",
            "\n",
            "Evaluating KNN on raw features:\n",
            "Using k=54 for KNN classification\n",
            "  Validation accuracy (raw features): 0.6110\n",
            "  Test accuracy (raw features): 0.5910\n",
            "\n",
            "Evaluating KNN on TabPFN embeddings:\n",
            "  Using 3000 training labels, 1000 validation labels, 1000 test labels\n",
            "  Label distribution in train: (array([0, 1]), array([1468, 1532]))\n",
            "  Validation accuracy (standardized embeddings): 0.6840\n",
            "  Test accuracy (standardized embeddings): 0.6970\n",
            "\n",
            "Evaluating KNN on PCA-reduced TabPFN embeddings:\n",
            "  Testing PCA components: [2, 5, 10, 20, 50, 100]\n",
            "  PCA components=2: Val accuracy=0.5720, Test accuracy=0.5860, Variance explained=0.4500\n",
            "  PCA components=5: Val accuracy=0.6230, Test accuracy=0.6750, Variance explained=0.7517\n",
            "  PCA components=10: Val accuracy=0.6530, Test accuracy=0.6870, Variance explained=0.8950\n",
            "  PCA components=20: Val accuracy=0.6790, Test accuracy=0.6950, Variance explained=0.9717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing datasets:  60%|██████    | 3/5 [29:04<20:25, 612.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PCA components=50: Val accuracy=0.6820, Test accuracy=0.6960, Variance explained=0.9964\n",
            "  PCA components=100: Val accuracy=0.6840, Test accuracy=0.6970, Variance explained=0.9995\n",
            "  Best PCA results: components=100, val_accuracy=0.6840, test_accuracy=0.6970\n",
            "\n",
            "\n",
            "=== SUMMARY OF RESULTS ===\n",
            "  dataset  full_test_accuracy  knn_raw_test_accuracy  knn_emb_test_accuracy  \\\n",
            "0   Higgs            0.714453                  0.591                  0.697   \n",
            "\n",
            "   knn_pca_best_components  knn_pca_test_accuracy  pca_variance_explained  \n",
            "0                      100                  0.697                0.999477  \n",
            "\n",
            "==================================================\n",
            "Analyzing dataset: airlines\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e378f9beba3e>:25: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
            "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes:\n",
            "  Train: (377568, 7), (array([0, 1]), array([209214, 168354]))\n",
            "  Val:   (80907, 7), (array([0, 1]), array([44802, 36105]))\n",
            "  Test:  (80908, 7), (array([0, 1]), array([45103, 35805]))\n",
            "Randomly selected 3000 samples from 377568 training samples\n",
            "Fitting TabPFN on 3000 samples\n",
            "Error during validation prediction: CUDA error: invalid configuration argument\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Error during test prediction: CUDA error: invalid configuration argument\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "TabPFN on initial training set:\n",
            "  Validation accuracy: 0.6353\n",
            "  Test accuracy: 0.6348\n",
            "Extracting embeddings from TabPFN\n",
            "Embeddings shapes - Train: (8, 3000, 192), Val: (8, 1000, 192), Test: (8, 1000, 192)\n",
            "Averaging embeddings across ensemble members\n",
            "Processed embedding shapes - Train: (3000, 192), Val: (1000, 192), Test: (1000, 192)\n",
            "\n",
            "Evaluating KNN on raw features:\n",
            "Using k=54 for KNN classification\n",
            "  Validation accuracy (raw features): 0.6170\n",
            "  Test accuracy (raw features): 0.5920\n",
            "\n",
            "Evaluating KNN on TabPFN embeddings:\n",
            "  Using 3000 training labels, 1000 validation labels, 1000 test labels\n",
            "  Label distribution in train: (array([0, 1]), array([1679, 1321]))\n",
            "  Validation accuracy (standardized embeddings): 0.6350\n",
            "  Test accuracy (standardized embeddings): 0.6250\n",
            "\n",
            "Evaluating KNN on PCA-reduced TabPFN embeddings:\n",
            "  Testing PCA components: [2, 5, 10, 20, 50, 100]\n",
            "  PCA components=2: Val accuracy=0.6180, Test accuracy=0.5950, Variance explained=0.5418\n",
            "  PCA components=5: Val accuracy=0.6230, Test accuracy=0.6140, Variance explained=0.7904\n",
            "  PCA components=10: Val accuracy=0.6210, Test accuracy=0.6210, Variance explained=0.9158\n",
            "  PCA components=20: Val accuracy=0.6400, Test accuracy=0.6260, Variance explained=0.9766\n",
            "  PCA components=50: Val accuracy=0.6330, Test accuracy=0.6240, Variance explained=0.9970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing datasets:  80%|████████  | 4/5 [31:20<07:04, 424.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PCA components=100: Val accuracy=0.6360, Test accuracy=0.6250, Variance explained=0.9996\n",
            "  Best PCA results: components=20, val_accuracy=0.6400, test_accuracy=0.6260\n",
            "\n",
            "\n",
            "=== SUMMARY OF RESULTS ===\n",
            "    dataset  full_test_accuracy  knn_raw_test_accuracy  knn_emb_test_accuracy  \\\n",
            "0  airlines             0.63477                  0.592                  0.625   \n",
            "\n",
            "   knn_pca_best_components  knn_pca_test_accuracy  pca_variance_explained  \n",
            "0                       20                  0.626                 0.97656  \n",
            "\n",
            "==================================================\n",
            "Analyzing dataset: albert\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\", line 1168, in _get_dataset_description\n",
            "    with description_file.open(encoding=\"utf8\") as fh:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/pathlib.py\", line 1044, in open\n",
            "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/openml/org/openml/www/datasets/189356/description.xml'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-10-e378f9beba3e>\", line 416, in main\n",
            "    result = analyze_dataset(dataset_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-e378f9beba3e>\", line 249, in analyze_dataset\n",
            "    X, y, categorical_indicator, attribute_names, full_name = load_dataset(dataset_name)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-e378f9beba3e>\", line 15, in load_dataset\n",
            "    dataset = openml.datasets.get_dataset(189356)  # Albert dataset\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\", line 590, in get_dataset\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\", line 556, in get_dataset\n",
            "    description = _get_dataset_description(did_cache_dir, dataset_id)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\", line 1173, in _get_dataset_description\n",
            "    dataset_xml = openml._api_calls._perform_api_call(url_extension, \"get\")\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/_api_calls.py\", line 120, in _perform_api_call\n",
            "    response = __read_url(url, request_method, data)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/_api_calls.py\", line 342, in __read_url\n",
            "    return _send_request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/_api_calls.py\", line 413, in _send_request\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/_api_calls.py\", line 387, in _send_request\n",
            "    __check_response(response=response, url=url, file_elements=files)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openml/_api_calls.py\", line 452, in __check_response\n",
            "    raise __parse_server_exception(response, url, file_elements=file_elements)\n",
            "openml.exceptions.OpenMLServerNoResult: https://www.openml.org/api/v1/xml/data/189356 returned code 111: Unknown dataset\n",
            "Processing datasets: 100%|██████████| 5/5 [31:20<00:00, 376.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing dataset albert: https://www.openml.org/api/v1/xml/data/189356 returned code 111: Unknown dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset names\n",
        "    dataset_names = ['har', 'volkert', 'higgs', 'airlines', 'albert']\n",
        "\n",
        "    # Store results\n",
        "    results = []\n",
        "\n",
        "    # Process each dataset\n",
        "    for dataset_name in tqdm(dataset_names, desc=\"Processing datasets\"):\n",
        "        try:\n",
        "            result = analyze_dataset(dataset_name)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing dataset {dataset_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Create summary dataframe\n",
        "    results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "naei0jDKxxaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "FmjVGEhRuxyW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}